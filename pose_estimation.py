# -*- coding: utf-8 -*-
"""Pose_estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dAB-OMSpf1YxW_G_OB_HEmpXic45u-pY
"""

!pip install mediapipe opencv-python

#Connect Google Drive
from google.colab import drive
drive._mount('/content/drive')

import mediapipe as mp
import cv2
import time
import numpy as np
import pandas as pd
import os
from google.colab.patches import cv2_imshow
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils # For drawing keypoints
points = mpPose.PoseLandmark # Landmarks
path = "/content/drive/MyDrive/plank" # enter dataset path
data = []
for p in points:
        x = str(p)[13:]
        data.append(x + "_x")
        data.append(x + "_y")
        data.append(x + "_z")
        data.append(x + "_vis")
data = pd.DataFrame(columns = data) # Empty dataset

count = 0

for img in os.listdir(path):

        temp = []

        img = cv2.imread(path + "/" + img)

        imageWidth, imageHeight = img.shape[:2]

        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        blackie = np.zeros(img.shape) # Blank image

        results = pose.process(imgRGB)

        if results.pose_landmarks:

                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image

                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie

                landmarks = results.pose_landmarks.landmark

                for i,j in zip(points,landmarks):

                        temp = temp + [j.x, j.y, j.z, j.visibility]

                data.loc[count] = temp

                count +=1

        cv2_imshow( img)

        cv2_imshow(blackie)

        cv2.waitKey(100)

data.to_csv("dataset3.csv") # save the data as a csv file

import mediapipe as mp
import cv2
import time
import numpy as np
import pandas as pd
import os
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils # For drawing keypoints
points = mpPose.PoseLandmark # Landmarks
path = "/content/drive/MyDrive/goddess" # enter dataset path
data = []
for p in points:
        x = str(p)[13:]
        data.append(x + "_x")
        data.append(x + "_y")
        data.append(x + "_z")
        data.append(x + "_vis")
data = pd.DataFrame(columns = data) # Empty dataset

count = 0

for img in os.listdir(path):

        temp = []

        img = cv2.imread(path + "/" + img)

        imageWidth, imageHeight = img.shape[:2]

        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        blackie = np.zeros(img.shape) # Blank image

        results = pose.process(imgRGB)

        if results.pose_landmarks:

                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image

                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie

                landmarks = results.pose_landmarks.landmark

                for i,j in zip(points,landmarks):

                        temp = temp + [j.x, j.y, j.z, j.visibility]

                data.loc[count] = temp

                count +=1

        cv2_imshow( img)

        cv2_imshow(blackie)

        cv2.waitKey(100)

data.to_csv("dataset4.csv") # save the data as a csv file

import mediapipe as mp
import cv2
import time
import numpy as np
import pandas as pd
import os
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils # For drawing keypoints
points = mpPose.PoseLandmark # Landmarks
path = "/content/drive/MyDrive/downdog" # enter dataset path
data = []
for p in points:
        x = str(p)[13:]
        data.append(x + "_x")
        data.append(x + "_y")
        data.append(x + "_z")
        data.append(x + "_vis")
data = pd.DataFrame(columns = data) # Empty dataset

count = 0

for img in os.listdir(path):

        temp = []

        img = cv2.imread(path + "/" + img)

        imageWidth, imageHeight = img.shape[:2]

        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        blackie = np.zeros(img.shape) # Blank image

        results = pose.process(imgRGB)

        if results.pose_landmarks:

                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image

                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie

                landmarks = results.pose_landmarks.landmark

                for i,j in zip(points,landmarks):

                        temp = temp + [j.x, j.y, j.z, j.visibility]

                data.loc[count] = temp

                count +=1

        cv2_imshow( img)

        cv2_imshow(blackie)

        cv2.waitKey(100)

data.to_csv("dataset5.csv") # save the data as a csv file

import mediapipe as mp
import cv2
import time
import numpy as np
import pandas as pd
import os
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils # For drawing keypoints
points = mpPose.PoseLandmark # Landmarks
path = "/content/drive/MyDrive/tree" # enter dataset path
data = []
for p in points:
        x = str(p)[13:]
        data.append(x + "_x")
        data.append(x + "_y")
        data.append(x + "_z")
        data.append(x + "_vis")
data = pd.DataFrame(columns = data) # Empty dataset

count = 0

for img in os.listdir(path):

        temp = []

        img = cv2.imread(path + "/" + img)

        imageWidth, imageHeight = img.shape[:2]

        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        blackie = np.zeros(img.shape) # Blank image

        results = pose.process(imgRGB)

        if results.pose_landmarks:

                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image

                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie

                landmarks = results.pose_landmarks.landmark

                for i,j in zip(points,landmarks):

                        temp = temp + [j.x, j.y, j.z, j.visibility]

                data.loc[count] = temp

                count +=1

        cv2_imshow( img)

        cv2_imshow(blackie)

        cv2.waitKey(100)

data.to_csv("dataset6.csv") # save the data as a csv file

import mediapipe as mp
import cv2
import time
import numpy as np
import pandas as pd
import os
mpPose = mp.solutions.pose
pose = mpPose.Pose()
mpDraw = mp.solutions.drawing_utils # For drawing keypoints
points = mpPose.PoseLandmark # Landmarks
path = "/content/drive/MyDrive/warrior2" # enter dataset path
data = []
for p in points:
        x = str(p)[13:]
        data.append(x + "_x")
        data.append(x + "_y")
        data.append(x + "_z")
        data.append(x + "_vis")
data = pd.DataFrame(columns = data) # Empty dataset

count = 0

for img in os.listdir(path):

        temp = []

        img = cv2.imread(path + "/" + img)

        imageWidth, imageHeight = img.shape[:2]

        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        blackie = np.zeros(img.shape) # Blank image

        results = pose.process(imgRGB)

        if results.pose_landmarks:

                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image

                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie

                landmarks = results.pose_landmarks.landmark

                for i,j in zip(points,landmarks):

                        temp = temp + [j.x, j.y, j.z, j.visibility]

                data.loc[count] = temp

                count +=1

        cv2_imshow( img)

        cv2_imshow(blackie)

        cv2.waitKey(100)

data.to_csv("dataset7.csv") # save the data as a csv file

"""SVM(linear)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load data from CSV
data1 = pd.read_csv("/content/dataset3.csv")
data2 = pd.read_csv("/content/dataset4.csv")
data3=  pd.read_csv("/content/dataset5.csv")
data4=  pd.read_csv("/content/dataset6.csv")
data5=  pd.read_csv("/content/dataset7.csv")
# Add a 'target' column with class label 'plank' for data1
data1['target'] = 'plank'

# Add a 'target' column with class label 'goddess' for data2
data2['target'] = 'goddess'

data3['target'] = 'downdog'

data4['target'] = 'tree'

data5['target'] = 'warrior2'
# Concatenate data1 and data2 vertically
merged_data = pd.concat([data1, data2,data3,data4,data5], ignore_index=True)


# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='linear')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(poly)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load data from CSV
data1 = pd.read_csv("dataset3.csv")
data2 = pd.read_csv("dataset4.csv")
data3=  pd.read_csv("dataset5.csv")
data4=  pd.read_csv("dataset6.csv")
data5=  pd.read_csv("dataset7.csv")
# Add a 'target' column with class label 'plank' for data1
data1['target'] = 'plank'

# Add a 'target' column with class label 'goddess' for data2
data2['target'] = 'goddess'

data3['target'] = 'downdog'

data4['target'] = 'tree'

data5['target'] = 'warrior2'
# Concatenate data1 and data2 vertically
merged_data = pd.concat([data1, data2,data3,data4,data5], ignore_index=True)

# Save merged_data to a CSV file
merged_data.to_csv("final_dataset.csv", index=False)

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='poly')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(rbf)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load data from CSV
data1 = pd.read_csv("dataset3.csv")
data2 = pd.read_csv("dataset4.csv")
data3=  pd.read_csv("dataset5.csv")
data4=  pd.read_csv("dataset6.csv")
data5=  pd.read_csv("dataset7.csv")
# Add a 'target' column with class label 'plank' for data1
data1['target'] = 'plank'

# Add a 'target' column with class label 'goddess' for data2
data2['target'] = 'goddess'

data3['target'] = 'downdog'

data4['target'] = 'tree'

data5['target'] = 'warrior2'
# Concatenate data1 and data2 vertically
merged_data = pd.concat([data1, data2,data3,data4,data5], ignore_index=True)

# Save merged_data to a CSV file
merged_data.to_csv("final_dataset.csv", index=False)

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='rbf')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Random Forest"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

merged_data=pd.read_csv("final_dataset.csv")

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Guassian Naive Bayes"""

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

merged_data = pd.read_csv('final_dataset.csv')

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gaussian Naive Bayes model
model = GaussianNB()

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""KNN"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

merged_data = pd.read_csv("final_dataset.csv")

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize k-Nearest Neighbors (kNN) model
k = 3  # Set the number of neighbors
model = KNeighborsClassifier(n_neighbors=k)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Gradient Boosting"""

import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

merged_data = pd.read_csv('final_dataset.csv')

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gradient Boosting Classifier
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Normalization"""

merged_data.describe()

merged_data.info()

merged_data.isnull().sum()

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load your DataFrame
merged_data = pd.read_csv("final_dataset.csv")

# Identify numerical columns (excluding 'target' column)
numerical_columns = merged_data.select_dtypes(include=['float64', 'int64']).columns

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Apply Min-Max scaling to numerical columns
merged_data[numerical_columns] = scaler.fit_transform(merged_data[numerical_columns])

merged_data.to_csv('normalised_csv')
# Now 'data' contains normalized values for numerical columns
merged_data.head()

"""SVM(Linear)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='linear')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(Poly)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='poly')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(rbf)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='rbf')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Random Forest"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""KNN"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize k-Nearest Neighbors (kNN) model
k = 3  # Set the number of neighbors
model = KNeighborsClassifier(n_neighbors=k)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Guassian Naive Bayes"""

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report



# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gaussian Naive Bayes model
model = GaussianNB()

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Gradient Boosting"""

import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gradient Boosting Classifier
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""PCA"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load your preprocessed DataFrame (after normalization)
data = pd.read_csv("/content/normalised_csv")

# Separate features (X) and target (Y)
X = data.drop('target', axis=1)  # Features (excluding the target column)
Y = data['target']                # Target column

# Standardize the features (optional but recommended for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize PCA with desired number of components
n_components = 10  # Example: choose the number of principal components
pca = PCA(n_components=n_components)

# Apply PCA to the scaled features
X_pca = pca.fit_transform(X_scaled)

# Create a new DataFrame with PCA components
pca_columns = [f"PC{i+1}" for i in range(n_components)]  # Column names for PCA components
X_pca_df = pd.DataFrame(data=X_pca, columns=pca_columns)

# Concatenate PCA components with target column (Y)
merged_data = pd.concat([X_pca_df, Y], axis=1)

merged_data.head()

"""SVM(linear)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='linear')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(poly)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='poly')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""SVM(rbf)"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize SVM model
model = SVC(kernel='rbf')

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Random Forest"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""KNN"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize k-Nearest Neighbors (kNN) model
k = 3  # Set the number of neighbors
model = KNeighborsClassifier(n_neighbors=k)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Guassian Naive Bayes"""

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report



# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gaussian Naive Bayes model
model = GaussianNB()

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))

"""Gradient Boosting"""

import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Split data into features (X) and target (Y)
X = merged_data.iloc[:, :-1]  # Features are all columns except the last one
Y = merged_data['target']     # Target is the last column

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize Gradient Boosting Classifier
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the testing set
Y_pred = model.predict(X_test)

# Print classification report
print(classification_report(Y_test, Y_pred))